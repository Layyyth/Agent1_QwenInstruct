{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. setup and installation \n",
    "!pip install unsloth_zoo==2025.6.8\n",
    "!pip install git+https://github.com/unslothai/unsloth.git\n",
    "!pip install bitsandbytes>=0.41.0\n",
    "!pip install accelerate>=0.20.0\n",
    "!pip install peft>=0.4.0\n",
    "!pip install transformers>=4.32.0\n",
    "!pip install trl>=0.4.7\n",
    "!pip install protobuf==5.29.1 fsspec==2025.3.2 --upgrade --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a09d3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model and LoRA Adapter Setup\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# --- Configuration ---\n",
    "max_seq_length = 1024\n",
    "dtype = torch.float16  # Use float16 for training\n",
    "load_in_4bit = True\n",
    "model_name = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\"\n",
    "lora_rank = 32\n",
    "\n",
    "# --- Load Base Model and Tokenizer ---\n",
    "print(f\"Loading base model: {model_name}\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"✓ Model and tokenizer loaded successfully.\")\n",
    "\n",
    "# --- Configure LoRA for Fine-Tuning ---\n",
    "print(f\"\\nApplying LoRA adapter with rank={lora_rank}...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=lora_rank,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "print(\"✓ LoRA adapter configured. Model is ready for training.\")\n",
    "\n",
    "# --- Final Memory Check ---\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Memory Usage after setup:\")\n",
    "    allocated_gb = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved_gb = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    print(f\"  - Allocated: {allocated_gb:.2f} GB\")\n",
    "    print(f\"  - Reserved:  {reserved_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb7d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Cleaning and Preparation\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "# --- System Prompt and Formatting ---\n",
    "# This is the master prompt that defines the agent's behavior.\n",
    "SYSTEM_PROMPT = \"\"\"You are an automated banking customer service ticket analysis system. Your purpose is to parse a customer's request and structure it into a standardized JSON format for internal ticketing.\n",
    "\n",
    "You must perform the following actions:\n",
    "1. Carefully analyze the user's input to understand their intent and key details.\n",
    "2. Populate all fields in the JSON object based only on the user's text. Do not invent information.\n",
    "3. Adhere strictly to the defined categories for ticket_type, severity, and other categorical fields.\n",
    "4. If the user's request is NOT related to banking or financial services (e.g., tech support for a personal computer, dating advice), you MUST reject it by responding with {\"error\": \"Request is outside the banking support domain.\"}.\n",
    "5. Your entire response must be ONLY the JSON object, with no conversational text, apologies, or explanations.\n",
    "\n",
    "The required JSON format is:\n",
    "{\n",
    "    \"ticket_type\": \"complaint\" | \"inquiry\" | \"assistance\",\n",
    "    \"title\": \"A brief, descriptive summary of the user's issue.\",\n",
    "    \"description\": \"A more detailed description based on the user's full input.\",\n",
    "    \"severity\": \"low\" | \"medium\" | \"high\" | \"critical\",\n",
    "    \"department_impacted\": \"The most relevant bank department.\",\n",
    "    \"service_impacted\": \"The specific banking service affected.\",\n",
    "    \"supporting_documents\": \"Attached documents and files by the customer\",\n",
    "    \"preferred_communication\": \"preferred method to contact the customer\"\n",
    "}\"\"\"\n",
    "\n",
    "# Standard Alpaca prompt template\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "# --- Load and Clean the Dataset ---\n",
    "print(\"Loading and cleaning 'banking_complaints_dataset1k.json'...\")\n",
    "file_path = 'banking_complaints_dataset1k.json'\n",
    "cleaned_data = []\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for item in data:\n",
    "        output_str = item.get(\"output\", \"\")\n",
    "        \n",
    "        # Standardize non-JSON rejection messages into the correct JSON error format\n",
    "        if not output_str.strip().startswith('{'):\n",
    "            output_str = json.dumps({\"error\": \"Request is outside the banking support domain.\"})\n",
    "        else:\n",
    "            # Clean and standardize the JSON content\n",
    "            try:\n",
    "                output_json = json.loads(output_str)\n",
    "                \n",
    "                # Fix 1: 'ticket_type' from \"asking for assistance\" to \"assistance\"\n",
    "                if output_json.get(\"ticket_type\") == \"asking for assistance\":\n",
    "                    output_json[\"ticket_type\"] = \"assistance\"\n",
    "                \n",
    "                # Fix 2: 'severity' from a list to a string (take the first element)\n",
    "                if isinstance(output_json.get(\"severity\"), list):\n",
    "                    output_json[\"severity\"] = output_json[\"severity\"][0]\n",
    "                \n",
    "                # Fix 3: Remove non-standard keys like 'assistance_request'\n",
    "                output_json.pop(\"assistance_request\", None)\n",
    "                \n",
    "                # Remove random numeric values from title\n",
    "                if \"title\" in output_json:\n",
    "                    output_json[\"title\"] = re.sub(r' - \\$\\d+', '', output_json[\"title\"])\n",
    "\n",
    "                output_str = json.dumps(output_json)\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                # If it's malformed JSON, treat it as an error case\n",
    "                output_str = json.dumps({\"error\": \"Malformed JSON in original data.\"})\n",
    "\n",
    "        cleaned_data.append({\n",
    "            \"instruction\": SYSTEM_PROMPT,\n",
    "            \"input\": item.get(\"input\", \"\"),\n",
    "            \"output\": output_str\n",
    "        })\n",
    "        \n",
    "    print(f\"✓ Successfully loaded and cleaned {len(cleaned_data)} records.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"✗ ERROR: Dataset file not found at '{file_path}'. Please ensure the file exists.\")\n",
    "    # Stop execution if the data isn't available\n",
    "    raise\n",
    "\n",
    "# --- Split and Format Data ---\n",
    "train_data, temp_data = train_test_split(cleaned_data, test_size=0.2, random_state=42)\n",
    "eval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"\\nDataset splits:\")\n",
    "print(f\"  - Train: {len(train_data)} samples\")\n",
    "print(f\"  - Eval:  {len(eval_data)} samples\")\n",
    "print(f\"  - Test:  {len(test_data)} samples\")\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "# Format datasets into final text prompts\n",
    "train_texts = [alpaca_prompt.format(**item) + EOS_TOKEN for item in train_data]\n",
    "eval_texts = [alpaca_prompt.format(**item) + EOS_TOKEN for item in eval_data]\n",
    "\n",
    "# Create Hugging Face Dataset objects for the trainer\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "eval_dataset = Dataset.from_dict({\"text\": eval_texts})\n",
    "\n",
    "print(\"\\n✓ Datasets prepared for training.\")\n",
    "print(\"\\n--- Sample Formatted Training Prompt ---\")\n",
    "print(train_dataset[0]['text'][:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1dc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Training, Evaluation, and Inference with Validation & Repair Loop\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, TrainerCallback\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import gc\n",
    "import json # Ensure json is imported for validation\n",
    "\n",
    "# --- Clear GPU Memory ---\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# --- Callback for Metrics Tracking ---\n",
    "class EnhancedLossCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses, self.eval_losses = [], []\n",
    "        self.learning_rates, self.steps, self.eval_steps = [], [], []\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs:\n",
    "            if \"loss\" in logs:\n",
    "                self.train_losses.append(logs[\"loss\"])\n",
    "                self.steps.append(state.global_step)\n",
    "            if \"eval_loss\" in logs:\n",
    "                self.eval_losses.append(logs[\"eval_loss\"])\n",
    "                self.eval_steps.append(state.global_step)\n",
    "            if \"learning_rate\" in logs:\n",
    "                self.learning_rates.append(logs[\"learning_rate\"])\n",
    "                \n",
    "    def plot_metrics(self, save_path=\"training_metrics.png\"):\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)\n",
    "        ax1.plot(self.steps, self.train_losses, 'b-', label='Training Loss')\n",
    "        ax1.plot(self.eval_steps, self.eval_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "        ax1.set_ylabel('Loss'); ax1.set_title('Training & Validation Loss'); ax1.legend(); ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        ax2.plot(self.steps, self.learning_rates, 'g-')\n",
    "        ax2.set_xlabel('Steps'); ax2.set_ylabel('Learning Rate'); ax2.set_title('Learning Rate Schedule'); ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout(); plt.savefig(save_path, dpi=300); plt.show()\n",
    "        if self.eval_losses: print(f\"Final Validation Loss: {self.eval_losses[-1]:.4f}\")\n",
    "\n",
    "loss_callback = EnhancedLossCallback()\n",
    "\n",
    "# --- Training Arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./banking_assistant_v1\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=1.5e-4,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=25,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# --- Initialize and Run Trainer ---\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    args=training_args,\n",
    "    callbacks=[loss_callback],\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"✓ Training complete.\")\n",
    "\n",
    "# --- Post-Training ---\n",
    "print(\"\\nPlotting metrics...\")\n",
    "loss_callback.plot_metrics()\n",
    "\n",
    "print(\"\\nSaving final LoRA adapters...\")\n",
    "model.save_pretrained(\"banking_assistant_final_adapters\")\n",
    "tokenizer.save_pretrained(\"banking_assistant_final_adapters\")\n",
    "print(\"✓ Adapters saved to 'banking_assistant_final_adapters'.\")\n",
    "\n",
    "# --- Final Evaluation and Inference with Validation & Repair ---\n",
    "print(\"\\n\" + \"=\"*50 + \"\\nEVALUATION AND INFERENCE\\n\" + \"=\"*50)\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "def generate_and_validate_json(user_input, max_retries=1):\n",
    "    \"\"\"\n",
    "    Generates a JSON response from the model, validates it, and attempts to\n",
    "    repair it if it's invalid.\n",
    "    \"\"\"\n",
    "    prompt = alpaca_prompt.format(instruction=SYSTEM_PROMPT, input=user_input, output=\"\")\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "        outputs = model.generate(**inputs, max_new_tokens=300, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
    "        response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        try:\n",
    "            # Isolate the JSON part of the response\n",
    "            json_str = response_text.split(\"### Response:\")[1].strip()\n",
    "            # Attempt to parse the JSON\n",
    "            parsed_json = json.loads(json_str)\n",
    "            return parsed_json # Success!\n",
    "        except (json.JSONDecodeError, IndexError) as e:\n",
    "            print(f\"\\n- Attempt {attempt + 1} failed: Invalid JSON generated.\")\n",
    "            print(f\"  Error: {e}\")\n",
    "            print(f\"  Faulty Output: {json_str}\")\n",
    "            \n",
    "            if attempt >= max_retries:\n",
    "                print(\"- Max retries reached. Returning failure.\")\n",
    "                return {\"error\": \"Failed to generate valid JSON after multiple attempts.\"}\n",
    "            \n",
    "            # Construct a repair prompt\n",
    "            repair_instruction = (\n",
    "                \"The following JSON is invalid. Please fix it. \"\n",
    "                f\"The error was: {e}. Do not add any commentary, just provide the corrected JSON object.\"\n",
    "            )\n",
    "            prompt = alpaca_prompt.format(\n",
    "                instruction=repair_instruction,\n",
    "                input=json_str,\n",
    "                output=\"\"\n",
    "            )\n",
    "            print(\"- Retrying with a repair prompt...\")\n",
    "\n",
    "# --- Sample Predictions (Qualitative Check) ---\n",
    "print(\"\\n--- Running Sample Predictions with Validation & Repair ---\")\n",
    "test_cases = [\n",
    "    \"My debit card was declined at the ATM but I have sufficient funds.\",\n",
    "    \"Can you help me set up automatic bill payments?\",\n",
    "    \"Can you help me fix my laptop? It won't start.\", # Out-of-domain\n",
    "    \"Someone made unauthorized purchases with my credit card in another country.\"\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    print(f\"\\n{'='*20}\\nInput: {case}\")\n",
    "    validated_json = generate_and_validate_json(case)\n",
    "    print(\"\\nFinal Validated JSON Output:\")\n",
    "    print(json.dumps(validated_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3219ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Final Test Set Evaluation\n",
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"  FINAL MODEL EVALUATION ON HELD-OUT TEST SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- 1. Load the Best Fine-Tuned Model ---\n",
    "# This ensures we are using the model that performed best on the validation set during training.\n",
    "# If you've already run the training cell, the 'model' variable is already the best one.\n",
    "# If you are running this in a new session, uncomment the lines below:\n",
    "# print(\"Loading best model from checkpoint...\")\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name=\"banking_assistant_final_adapters\", # Or your final checkpoint directory\n",
    "#     max_seq_length=max_seq_length,\n",
    "#     dtype=dtype,\n",
    "#     load_in_4bit=load_in_4bit,\n",
    "# )\n",
    "# FastLanguageModel.for_inference(model)\n",
    "# print(\"✓ Model loaded for inference.\")\n",
    "\n",
    "\n",
    "# --- 2. Quantitative Evaluation: Perplexity and Loss ---\n",
    "# This measures how \"surprised\" the model is by the test data. Lower is better.\n",
    "\n",
    "def calculate_test_perplexity(model, tokenizer, test_data, batch_size=4):\n",
    "    \"\"\"Calculates loss and perplexity on the test set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Prepare the test texts\n",
    "    test_prompts = [alpaca_prompt.format(**item) + EOS_TOKEN for item in test_data]\n",
    "    \n",
    "    print(f\"\\nCalculating perplexity on {len(test_prompts)} test samples...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(test_prompts), batch_size), desc=\"Test Batches\"):\n",
    "            batch = test_prompts[i : i + batch_size]\n",
    "            inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_length).to(\"cuda\")\n",
    "            \n",
    "            # The labels are the input_ids themselves for language modeling loss\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            total_loss += outputs.loss.item() * len(batch)\n",
    "            \n",
    "    avg_loss = total_loss / len(test_prompts)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return avg_loss, perplexity\n",
    "\n",
    "# Run the quantitative evaluation\n",
    "test_loss, test_perplexity = calculate_test_perplexity(model, tokenizer, test_data)\n",
    "\n",
    "print(\"\\n--- Quantitative Metrics ---\")\n",
    "print(f\"✅ Test Set Loss: {test_loss:.4f}\")\n",
    "print(f\"✅ Test Set Perplexity: {test_perplexity:.4f}\")\n",
    "print(\"(Note: Perplexity measures how well the model predicts the next token. Lower is better.)\")\n",
    "\n",
    "\n",
    "# --- 3. Qualitative Evaluation: Accuracy of JSON Fields ---\n",
    "# This measures how well the model performs on the actual task.\n",
    "\n",
    "def evaluate_json_accuracy(model, tokenizer, test_data):\n",
    "    \"\"\"Generates responses for the test set and compares key JSON fields.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    correct_predictions = {\n",
    "        \"ticket_type\": 0,\n",
    "        \"severity\": 0,\n",
    "        \"department_impacted\": 0,\n",
    "        \"is_valid_json\": 0,\n",
    "        \"is_correct_error\": 0,\n",
    "    }\n",
    "    total_banking_requests = 0\n",
    "    total_error_requests = 0\n",
    "    failed_generations = []\n",
    "\n",
    "    print(f\"\\nCalculating JSON field accuracy on {len(test_data)} test samples...\")\n",
    "\n",
    "    for item in tqdm(test_data, desc=\"Evaluating JSON Accuracy\"):\n",
    "        # Prepare the prompt for inference (without the ground truth output)\n",
    "        prompt = alpaca_prompt.format(instruction=item[\"instruction\"], input=item[\"input\"], output=\"\")\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        # Generate the model's prediction\n",
    "        outputs = model.generate(**inputs, max_new_tokens=300, use_cache=True, pad_token_id=tokenizer.eos_token_id)\n",
    "        response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract the JSON part of the response\n",
    "        try:\n",
    "            predicted_json_str = response_text.split(\"### Response:\")[1].strip()\n",
    "            predicted_json = json.loads(predicted_json_str)\n",
    "            correct_predictions[\"is_valid_json\"] += 1\n",
    "        except (json.JSONDecodeError, IndexError):\n",
    "            failed_generations.append({\"input\": item[\"input\"], \"output\": response_text})\n",
    "            continue # Skip to next item if JSON is invalid\n",
    "\n",
    "        # Load the ground truth JSON\n",
    "        ground_truth_json = json.loads(item[\"output\"])\n",
    "\n",
    "        # Compare fields\n",
    "        if \"error\" in ground_truth_json:\n",
    "            total_error_requests += 1\n",
    "            if predicted_json.get(\"error\") == ground_truth_json.get(\"error\"):\n",
    "                correct_predictions[\"is_correct_error\"] += 1\n",
    "        else:\n",
    "            total_banking_requests += 1\n",
    "            if predicted_json.get(\"ticket_type\") == ground_truth_json.get(\"ticket_type\"):\n",
    "                correct_predictions[\"ticket_type\"] += 1\n",
    "            if predicted_json.get(\"severity\") == ground_truth_json.get(\"severity\"):\n",
    "                correct_predictions[\"severity\"] += 1\n",
    "            if predicted_json.get(\"department_impacted\") == ground_truth_json.get(\"department_impacted\"):\n",
    "                correct_predictions[\"department_impacted\"] += 1\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    accuracies = {\n",
    "        \"JSON Validation Rate\": (correct_predictions[\"is_valid_json\"] / len(test_data)) * 100,\n",
    "        \"Ticket Type Accuracy\": (correct_predictions[\"ticket_type\"] / total_banking_requests) * 100 if total_banking_requests > 0 else 0,\n",
    "        \"Severity Accuracy\": (correct_predictions[\"severity\"] / total_banking_requests) * 100 if total_banking_requests > 0 else 0,\n",
    "        \"Department Accuracy\": (correct_predictions[\"department_impacted\"] / total_banking_requests) * 100 if total_banking_requests > 0 else 0,\n",
    "        \"Error Handling Accuracy\": (correct_predictions[\"is_correct_error\"] / total_error_requests) * 100 if total_error_requests > 0 else 0,\n",
    "    }\n",
    "    \n",
    "    return accuracies, failed_generations\n",
    "\n",
    "# Run the qualitative evaluation\n",
    "accuracies, failed_generations = evaluate_json_accuracy(model, tokenizer, test_data)\n",
    "\n",
    "print(\"\\n--- Qualitative Metrics (Task-Specific Accuracy) ---\")\n",
    "for metric, value in accuracies.items():\n",
    "    print(f\"✅ {metric}: {value:.2f}%\")\n",
    "\n",
    "if failed_generations:\n",
    "    print(\"\\n--- Review of Failed Generations (Invalid JSON) ---\")\n",
    "    for i, failure in enumerate(failed_generations[:5]): # Print up to 5 failures\n",
    "        print(f\"\\nFailure #{i+1}:\")\n",
    "        print(f\"  Input: {failure['input']}\")\n",
    "        print(f\"  Model Output: {failure['output']}\")\n",
    "else:\n",
    "    print(\"\\n✅ No invalid JSON generations were found in the test set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10f341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Simple cell to upload LoRA adapters to Hugging Face\n",
    "\n",
    "from huggingface_hub import HfApi, login\n",
    "import os\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "\n",
    "# The local folder where your trained adapters are saved.\n",
    "ADAPTER_PATH = \"banking_assistant_final_adapters\" \n",
    "\n",
    "# The name of your repository on the Hugging Face Hub.\n",
    "HF_REPO_NAME = \"LaythAbuJafar/Agent1_Adapters\"\n",
    "\n",
    "# Your Hugging Face token with 'write' permissions.\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\" \n",
    "\n",
    "# --- 2. UPLOAD SCRIPT ---\n",
    "\n",
    "print(f\"--- Starting Upload to {HF_REPO_NAME} ---\")\n",
    "\n",
    "# Check if the local adapter path exists before trying to upload\n",
    "if not os.path.isdir(ADAPTER_PATH):\n",
    "    print(f\"❌ ERROR: The directory '{ADAPTER_PATH}' was not found.\")\n",
    "    print(\"Please make sure your training completed and saved the adapters to that folder.\")\n",
    "else:\n",
    "    try:\n",
    "        # Step 1: Login to Hugging Face\n",
    "        print(\"Logging in...\")\n",
    "        login(token=HF_TOKEN)\n",
    "        print(\"✓ Login successful.\")\n",
    "\n",
    "        # Step 2: Upload the folder\n",
    "        print(f\"Uploading files from '{ADAPTER_PATH}'...\")\n",
    "        api = HfApi()\n",
    "        api.upload_folder(\n",
    "            folder_path=ADAPTER_PATH,\n",
    "            repo_id=HF_REPO_NAME,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Upload fine-tuned LoRA adapters\"\n",
    "        )\n",
    "        print(f\"\\n✅ SUCCESS! Your adapters have been uploaded to:\")\n",
    "        print(f\"https://huggingface.co/{HF_REPO_NAME}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An error occurred during the upload process: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd7cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 Simple cell to merge LoRA adapters and upload the full model to Hugging Face\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from unsloth import FastLanguageModel\n",
    "from huggingface_hub import HfApi, login, create_repo\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "\n",
    "# The local folder where your trained adapters are saved.\n",
    "ADAPTER_PATH = \"banking_assistant_final_adapters\"\n",
    "\n",
    "# The local folder where the new merged model will be saved.\n",
    "MERGED_MODEL_PATH = \"banking_assistant_merged_final\"\n",
    "\n",
    "# The name of your repository on the Hugging Face Hub for the MERGED model.\n",
    "HF_REPO_NAME = \"LaythAbuJafar/Agent1_Merged\"\n",
    "\n",
    "# Your Hugging Face token with 'write' permissions.\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"\n",
    "\n",
    "# --- 2. MERGE AND UPLOAD SCRIPT ---\n",
    "\n",
    "print(\"--- Starting Model Merge and Upload Process ---\")\n",
    "\n",
    "# Check if the adapter path exists\n",
    "if not os.path.isdir(ADAPTER_PATH):\n",
    "    print(f\"❌ ERROR: The adapter directory '{ADAPTER_PATH}' was not found.\")\n",
    "    print(\"Please make sure your training completed and saved the adapters to that folder.\")\n",
    "else:\n",
    "    try:\n",
    "        # Step 1: Load the base model and apply the adapters\n",
    "        print(\"\\nStep 1: Loading base model and applying LoRA adapters...\")\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=ADAPTER_PATH,  # Load the adapters directly\n",
    "            max_seq_length=1024,\n",
    "            dtype=torch.float16,\n",
    "            load_in_4bit=True,\n",
    "        )\n",
    "        print(\"✓ Model and adapters loaded successfully.\")\n",
    "\n",
    "        # Step 2: Merge the adapters into the model\n",
    "        print(f\"\\nStep 2: Merging adapters into the model and saving to '{MERGED_MODEL_PATH}'...\")\n",
    "        # This saves the full model in 16-bit precision (float16)\n",
    "        model.save_pretrained_merged(MERGED_MODEL_PATH, tokenizer, save_method=\"merged_16bit\")\n",
    "        print(\"✓ Model merged and saved locally.\")\n",
    "\n",
    "        # Step 3: Login to Hugging Face\n",
    "        print(\"\\nStep 3: Logging in to Hugging Face...\")\n",
    "        login(token=HF_TOKEN)\n",
    "        print(\"✓ Login successful.\")\n",
    "\n",
    "        # Step 4: Create the repository and upload the merged model\n",
    "        print(f\"\\nStep 4: Uploading the merged model to '{HF_REPO_NAME}'...\")\n",
    "        print(\"⚠️ This will upload a large model (~15 GB). This may take a while.\")\n",
    "        \n",
    "        api = HfApi()\n",
    "        # Create the repo if it doesn't exist\n",
    "        create_repo(HF_REPO_NAME, repo_type=\"model\", exist_ok=True)\n",
    "        \n",
    "        # Upload the entire merged model folder\n",
    "        api.upload_folder(\n",
    "            folder_path=MERGED_MODEL_PATH,\n",
    "            repo_id=HF_REPO_NAME,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Upload merged 16-bit model\"\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n✅ SUCCESS! Your merged model has been uploaded to:\")\n",
    "        print(f\"https://huggingface.co/{HF_REPO_NAME}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ An error occurred during the process: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f05e015d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: llama.cpp/build/bin/llama-quantize Agent1_Merged.gguf Agent1_Merged.q4_K_M.gguf q4_K_M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "main: build = 5893 (0f4c6ec0)\n",
      "main: built with cc (Ubuntu 13.3.0-6ubuntu2~24.04) 13.3.0 for x86_64-linux-gnu\n",
      "main: quantizing 'Agent1_Merged.gguf' to 'Agent1_Merged.q4_K_M.gguf' as Q4_K_M\n",
      "llama_model_loader: loaded meta data with 23 key-value pairs and 339 tensors from Agent1_Merged.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Agent1_Merged\n",
      "llama_model_loader: - kv   3:                         general.size_label str              = 7.6B\n",
      "llama_model_loader: - kv   4:                          qwen2.block_count u32              = 28\n",
      "llama_model_loader: - kv   5:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   6:                     qwen2.embedding_length u32              = 3584\n",
      "llama_model_loader: - kv   7:                  qwen2.feed_forward_length u32              = 18944\n",
      "llama_model_loader: - kv   8:                 qwen2.attention.head_count u32              = 28\n",
      "llama_model_loader: - kv   9:              qwen2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  10:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  13:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 151654\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|im_start|>...\n",
      "llama_model_loader: - type  f32:  141 tensors\n",
      "llama_model_loader: - type  f16:  198 tensors\n",
      "[   1/ 339]                        output.weight - [ 3584, 152064,     1,     1], type =    f16, converting to q6_K .. size =  1039.50 MiB ->   426.36 MiB\n",
      "[   2/ 339]                   output_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[   3/ 339]                    token_embd.weight - [ 3584, 152064,     1,     1], type =    f16, converting to q4_K .. size =  1039.50 MiB ->   292.36 MiB\n",
      "[   4/ 339]                    blk.0.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[   5/ 339]                  blk.0.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[   6/ 339]               blk.0.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[   7/ 339]             blk.0.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[   8/ 339]                    blk.0.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[   9/ 339]                  blk.0.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  10/ 339]                    blk.0.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  11/ 339]                  blk.0.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[  12/ 339]                blk.0.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[  13/ 339]                blk.0.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  14/ 339]                blk.0.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  15/ 339]                  blk.0.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  16/ 339]                    blk.1.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  17/ 339]                  blk.1.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  18/ 339]               blk.1.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  19/ 339]             blk.1.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  20/ 339]                    blk.1.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  21/ 339]                  blk.1.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  22/ 339]                    blk.1.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  23/ 339]                  blk.1.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[  24/ 339]                blk.1.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[  25/ 339]                blk.1.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  26/ 339]                blk.1.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  27/ 339]                  blk.1.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  28/ 339]                    blk.2.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  29/ 339]                  blk.2.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  30/ 339]               blk.2.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  31/ 339]             blk.2.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  32/ 339]                    blk.2.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  33/ 339]                  blk.2.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  34/ 339]                    blk.2.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  35/ 339]                  blk.2.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[  36/ 339]                blk.2.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[  37/ 339]                blk.2.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  38/ 339]                blk.2.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  39/ 339]                  blk.2.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  40/ 339]                    blk.3.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  41/ 339]                  blk.3.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  42/ 339]               blk.3.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  43/ 339]             blk.3.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  44/ 339]                    blk.3.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  45/ 339]                  blk.3.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  46/ 339]                    blk.3.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  47/ 339]                  blk.3.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  48/ 339]                blk.3.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  49/ 339]                blk.3.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  50/ 339]                blk.3.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  51/ 339]                  blk.3.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  52/ 339]                    blk.4.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  53/ 339]                  blk.4.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  54/ 339]               blk.4.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  55/ 339]             blk.4.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  56/ 339]                    blk.4.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  57/ 339]                  blk.4.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  58/ 339]                    blk.4.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  59/ 339]                  blk.4.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  60/ 339]                blk.4.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  61/ 339]                blk.4.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  62/ 339]                blk.4.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  63/ 339]                  blk.4.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  64/ 339]                    blk.5.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  65/ 339]                  blk.5.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  66/ 339]               blk.5.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  67/ 339]             blk.5.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  68/ 339]                    blk.5.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  69/ 339]                  blk.5.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  70/ 339]                    blk.5.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  71/ 339]                  blk.5.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[  72/ 339]                blk.5.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[  73/ 339]                blk.5.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  74/ 339]                blk.5.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  75/ 339]                  blk.5.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  76/ 339]                    blk.6.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  77/ 339]                  blk.6.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  78/ 339]               blk.6.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  79/ 339]             blk.6.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  80/ 339]                    blk.6.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  81/ 339]                  blk.6.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  82/ 339]                    blk.6.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  83/ 339]                  blk.6.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  84/ 339]                blk.6.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  85/ 339]                blk.6.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  86/ 339]                blk.6.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  87/ 339]                  blk.6.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  88/ 339]                    blk.7.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  89/ 339]                  blk.7.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  90/ 339]               blk.7.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  91/ 339]             blk.7.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  92/ 339]                    blk.7.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  93/ 339]                  blk.7.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[  94/ 339]                    blk.7.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[  95/ 339]                  blk.7.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[  96/ 339]                blk.7.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  97/ 339]                blk.7.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[  98/ 339]                blk.7.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[  99/ 339]                  blk.7.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 100/ 339]                    blk.8.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 101/ 339]                  blk.8.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 102/ 339]               blk.8.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 103/ 339]             blk.8.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 104/ 339]                    blk.8.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 105/ 339]                  blk.8.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 106/ 339]                    blk.8.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 107/ 339]                  blk.8.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 108/ 339]                blk.8.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[ 109/ 339]                blk.8.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 110/ 339]                blk.8.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 111/ 339]                  blk.8.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 112/ 339]                    blk.9.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 113/ 339]                  blk.9.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 114/ 339]               blk.9.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 115/ 339]             blk.9.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 116/ 339]                    blk.9.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 117/ 339]                  blk.9.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 118/ 339]                    blk.9.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 119/ 339]                  blk.9.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 120/ 339]                blk.9.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 121/ 339]                blk.9.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 122/ 339]                blk.9.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 123/ 339]                  blk.9.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 124/ 339]                   blk.10.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 125/ 339]                 blk.10.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 126/ 339]              blk.10.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 127/ 339]            blk.10.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 128/ 339]                   blk.10.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 129/ 339]                 blk.10.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 130/ 339]                   blk.10.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 131/ 339]                 blk.10.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 132/ 339]               blk.10.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 133/ 339]               blk.10.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 134/ 339]               blk.10.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 135/ 339]                 blk.10.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 136/ 339]                   blk.11.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 137/ 339]                 blk.11.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 138/ 339]              blk.11.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 139/ 339]            blk.11.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 140/ 339]                   blk.11.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 141/ 339]                 blk.11.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 142/ 339]                   blk.11.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 143/ 339]                 blk.11.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 144/ 339]               blk.11.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[ 145/ 339]               blk.11.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 146/ 339]               blk.11.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 147/ 339]                 blk.11.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 148/ 339]                   blk.12.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 149/ 339]                 blk.12.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 150/ 339]              blk.12.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 151/ 339]            blk.12.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 152/ 339]                   blk.12.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 153/ 339]                 blk.12.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 154/ 339]                   blk.12.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 155/ 339]                 blk.12.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 156/ 339]               blk.12.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 157/ 339]               blk.12.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 158/ 339]               blk.12.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 159/ 339]                 blk.12.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 160/ 339]                   blk.13.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 161/ 339]                 blk.13.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 162/ 339]              blk.13.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 163/ 339]            blk.13.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 164/ 339]                   blk.13.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 165/ 339]                 blk.13.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 166/ 339]                   blk.13.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 167/ 339]                 blk.13.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 168/ 339]               blk.13.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 169/ 339]               blk.13.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 170/ 339]               blk.13.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 171/ 339]                 blk.13.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 172/ 339]                   blk.14.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 173/ 339]                 blk.14.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 174/ 339]              blk.14.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 175/ 339]            blk.14.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 176/ 339]                   blk.14.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 177/ 339]                 blk.14.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 178/ 339]                   blk.14.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 179/ 339]                 blk.14.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 180/ 339]               blk.14.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[ 181/ 339]               blk.14.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 182/ 339]               blk.14.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 183/ 339]                 blk.14.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 184/ 339]                   blk.15.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 185/ 339]                 blk.15.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 186/ 339]              blk.15.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 187/ 339]            blk.15.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 188/ 339]                   blk.15.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 189/ 339]                 blk.15.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 190/ 339]                   blk.15.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 191/ 339]                 blk.15.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 192/ 339]               blk.15.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 193/ 339]               blk.15.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 194/ 339]               blk.15.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 195/ 339]                 blk.15.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 196/ 339]                   blk.16.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 197/ 339]                 blk.16.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 198/ 339]              blk.16.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 199/ 339]            blk.16.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 200/ 339]                   blk.16.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 201/ 339]                 blk.16.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 202/ 339]                   blk.16.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 203/ 339]                 blk.16.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 204/ 339]               blk.16.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 205/ 339]               blk.16.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 206/ 339]               blk.16.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 207/ 339]                 blk.16.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 208/ 339]                   blk.17.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 209/ 339]                 blk.17.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 210/ 339]              blk.17.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 211/ 339]            blk.17.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 212/ 339]                   blk.17.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 213/ 339]                 blk.17.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 214/ 339]                   blk.17.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 215/ 339]                 blk.17.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 216/ 339]               blk.17.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[ 217/ 339]               blk.17.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 218/ 339]               blk.17.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 219/ 339]                 blk.17.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 220/ 339]                   blk.18.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 221/ 339]                 blk.18.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 222/ 339]              blk.18.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 223/ 339]            blk.18.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 224/ 339]                   blk.18.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 225/ 339]                 blk.18.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 226/ 339]                   blk.18.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 227/ 339]                 blk.18.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 228/ 339]               blk.18.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 229/ 339]               blk.18.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 230/ 339]               blk.18.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 231/ 339]                 blk.18.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 232/ 339]                   blk.19.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 233/ 339]                 blk.19.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 234/ 339]              blk.19.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 235/ 339]            blk.19.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 236/ 339]                   blk.19.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 237/ 339]                 blk.19.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 238/ 339]                   blk.19.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 239/ 339]                 blk.19.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 240/ 339]               blk.19.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 241/ 339]               blk.19.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 242/ 339]               blk.19.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 243/ 339]                 blk.19.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 244/ 339]                   blk.20.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 245/ 339]                 blk.20.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 246/ 339]              blk.20.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 247/ 339]            blk.20.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 248/ 339]                   blk.20.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 249/ 339]                 blk.20.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 250/ 339]                   blk.20.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 251/ 339]                 blk.20.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 252/ 339]               blk.20.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[ 253/ 339]               blk.20.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 254/ 339]               blk.20.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 255/ 339]                 blk.20.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 256/ 339]                   blk.21.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 257/ 339]                 blk.21.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 258/ 339]              blk.21.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 259/ 339]            blk.21.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 260/ 339]                   blk.21.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 261/ 339]                 blk.21.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 262/ 339]                   blk.21.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 263/ 339]                 blk.21.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 264/ 339]               blk.21.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 265/ 339]               blk.21.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 266/ 339]               blk.21.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 267/ 339]                 blk.21.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 268/ 339]                   blk.22.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 269/ 339]                 blk.22.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 270/ 339]              blk.22.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 271/ 339]            blk.22.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 272/ 339]                   blk.22.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 273/ 339]                 blk.22.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 274/ 339]                   blk.22.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 275/ 339]                 blk.22.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 276/ 339]               blk.22.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 277/ 339]               blk.22.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 278/ 339]               blk.22.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 279/ 339]                 blk.22.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 280/ 339]                   blk.23.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 281/ 339]                 blk.23.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 282/ 339]              blk.23.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 283/ 339]            blk.23.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 284/ 339]                   blk.23.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 285/ 339]                 blk.23.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 286/ 339]                   blk.23.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 287/ 339]                 blk.23.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 288/ 339]               blk.23.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[ 289/ 339]               blk.23.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 290/ 339]               blk.23.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 291/ 339]                 blk.23.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 292/ 339]                   blk.24.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 293/ 339]                 blk.24.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 294/ 339]              blk.24.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 295/ 339]            blk.24.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 296/ 339]                   blk.24.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 297/ 339]                 blk.24.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 298/ 339]                   blk.24.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 299/ 339]                 blk.24.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 300/ 339]               blk.24.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[ 301/ 339]               blk.24.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 302/ 339]               blk.24.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 303/ 339]                 blk.24.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 304/ 339]                   blk.25.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 305/ 339]                 blk.25.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 306/ 339]              blk.25.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 307/ 339]            blk.25.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 308/ 339]                   blk.25.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 309/ 339]                 blk.25.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 310/ 339]                   blk.25.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 311/ 339]                 blk.25.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 312/ 339]               blk.25.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[ 313/ 339]               blk.25.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 314/ 339]               blk.25.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 315/ 339]                 blk.25.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 316/ 339]                   blk.26.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 317/ 339]                 blk.26.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 318/ 339]              blk.26.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 319/ 339]            blk.26.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 320/ 339]                   blk.26.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 321/ 339]                 blk.26.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 322/ 339]                   blk.26.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 323/ 339]                 blk.26.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 324/ 339]               blk.26.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[ 325/ 339]               blk.26.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 326/ 339]               blk.26.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 327/ 339]                 blk.26.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 328/ 339]                   blk.27.attn_k.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 329/ 339]                 blk.27.attn_k.weight - [ 3584,   512,     1,     1], type =    f16, converting to q4_K .. size =     3.50 MiB ->     0.98 MiB\n",
      "[ 330/ 339]              blk.27.attn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 331/ 339]            blk.27.attn_output.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 332/ 339]                   blk.27.attn_q.bias - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 333/ 339]                 blk.27.attn_q.weight - [ 3584,  3584,     1,     1], type =    f16, converting to q4_K .. size =    24.50 MiB ->     6.89 MiB\n",
      "[ 334/ 339]                   blk.27.attn_v.bias - [  512,     1,     1,     1], type =    f32, size =    0.002 MB\n",
      "[ 335/ 339]                 blk.27.attn_v.weight - [ 3584,   512,     1,     1], type =    f16, converting to q6_K .. size =     3.50 MiB ->     1.44 MiB\n",
      "[ 336/ 339]               blk.27.ffn_down.weight - [18944,  3584,     1,     1], type =    f16, converting to q6_K .. size =   129.50 MiB ->    53.12 MiB\n",
      "[ 337/ 339]               blk.27.ffn_gate.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "[ 338/ 339]               blk.27.ffn_norm.weight - [ 3584,     1,     1,     1], type =    f32, size =    0.014 MB\n",
      "[ 339/ 339]                 blk.27.ffn_up.weight - [ 3584, 18944,     1,     1], type =    f16, converting to q4_K .. size =   129.50 MiB ->    36.42 MiB\n",
      "llama_model_quantize_impl: model size  = 14526.27 MB\n",
      "llama_model_quantize_impl: quant size  =  4460.45 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "main: quantize time = 162722.84 ms\n",
      "main:    total time = 162722.84 ms\n",
      "✅ Success! Quantized model saved to:\n",
      "Agent1_Merged.q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "# 8 . Quantize \n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "model_path = r\"Agent1_Merged.gguf\"\n",
    "quant_type = \"q4_K_M\"  # Options: q4_0, q5_1, q8_0, etc.\n",
    "llama_cpp_bin_dir = r\"llama.cpp/build/bin\"\n",
    "output_path = os.path.splitext(model_path)[0] + f\".{quant_type}.gguf\"\n",
    "\n",
    "# === MAIN LOGIC ===\n",
    "quantize_exe = os.path.join(llama_cpp_bin_dir, \"llama-quantize\")\n",
    "if not os.path.exists(quantize_exe):\n",
    "    raise FileNotFoundError(f\"llama-quantize.exe not found at: {quantize_exe}\")\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at: {model_path}\")\n",
    "\n",
    "cmd = [quantize_exe, model_path, output_path, quant_type]\n",
    "print(f\"Running: {' '.join(cmd)}\")\n",
    "\n",
    "result = subprocess.run(cmd)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"✅ Success! Quantized model saved to:\\n{output_path}\")\n",
    "else:\n",
    "    print(f\"❌ Quantization failed with code {result.returncode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37fc1d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50d658ea667499ea9a1d581c817a46c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Agent1_Merged.q4_K_M.gguf:   0%|          | 0.00/4.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upload complete!\n"
     ]
    }
   ],
   "source": [
    "# 9 upload quantized model to hf\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "local_path = r\"Agent1_Merged.q4_K_M.gguf\"\n",
    "repo_id = \"LaythAbuJafar/Agent1_GGUF_Q\"\n",
    "target_path_in_repo = \"Agent1_Merged.q4_K_M.gguf\"\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=local_path,\n",
    "    path_in_repo=target_path_in_repo,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(\"✅ Upload complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

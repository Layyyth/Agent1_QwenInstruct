{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa42816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. setup and installation \n",
    "!pip install unsloth_zoo==2025.6.8\n",
    "!pip install git+https://github.com/unslothai/unsloth.git\n",
    "!pip install bitsandbytes>=0.41.0\n",
    "!pip install accelerate>=0.20.0\n",
    "!pip install peft>=0.4.0\n",
    "!pip install transformers>=4.32.0\n",
    "!pip install trl>=0.4.7\n",
    "!pip install protobuf==5.29.1 fsspec==2025.3.2 --upgrade --force-reinstall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a8ce8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Model and Tokenizer\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Model and tokenizer configuration\n",
    "max_seq_length = 2048\n",
    "dtype = None # Auto-detect\n",
    "load_in_4bit = True # Use 4-bit quantization\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f4018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Cell 3: Data Preparation with System Prompt Integration\n",
    "\n",
    "# Step 1: Set environment variable BEFORE any imports\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Step 2: Import required libraries\n",
    "import torch\n",
    "import gc\n",
    "import json\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 3: Check GPU status\n",
    "print(\"GPU Status Check:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ CUDA available\")\n",
    "    print(f\"✓ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"✓ Initial memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"✗ No GPU available!\")\n",
    "    \n",
    "# Step 4: Import Unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Step 5: Load model with conservative settings\n",
    "print(\"\\nLoading model with conservative settings...\")\n",
    "max_seq_length = 1024  # Reduced for safety\n",
    "dtype = torch.float16  # Explicit dtype\n",
    "load_in_4bit = True\n",
    "\n",
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"unsloth/Qwen2.5-7B-Instruct\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        device_map=\"sequential\",  # More conservative than \"auto\"\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    print(\"✓ Model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading model: {e}\")\n",
    "    print(\"\\nTrying alternative: base Qwen model...\")\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "\n",
    "# Define your SYSTEM PROMPT\n",
    "SYSTEM_PROMPT = \"\"\"You are an automated banking customer service ticket analysis system. Your purpose is to parse a customer's request and structure it into a standardized JSON format for internal ticketing.\n",
    "\n",
    "You must perform the following actions:\n",
    "1. Carefully analyze the user's input to understand their intent and key details.\n",
    "2. Populate all fields in the JSON object based only on the user's text. Do not invent information.\n",
    "3. Adhere strictly to the defined categories for ticket_type, severity, and other categorical fields.\n",
    "4. If the user's request is NOT related to banking or financial services (e.g., tech support for a personal computer, dating advice), you MUST reject it by responding with {\"error\": \"Request is outside the banking support domain.\"}.\n",
    "5. Your entire response must be ONLY the JSON object, with no conversational text, apologies, or explanations.\n",
    "\n",
    "The required JSON format is:\n",
    "{\n",
    "    \"ticket_type\": \"complaint\" | \"inquiry\" | \"assistance\",\n",
    "    \"title\": \"A brief, descriptive summary of the user's issue.\",\n",
    "    \"description\": \"A more detailed description based on the user's full input.\",\n",
    "    \"severity\": \"low\" | \"medium\" | \"high\" | \"critical\",\n",
    "    \"department_impacted\": \"The most relevant bank department.\",\n",
    "    \"service_impacted\": \"The specific banking service affected.\",\n",
    "    \"supporting_documents\": \"Attached documents and files by the customer\",\n",
    "    \"preferred_communication\": \"preferred method to contact the customer\"\n",
    "}\"\"\"\n",
    "\n",
    "# Step 6: Define Standard Alpaca prompt with proper formatting\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "# Step 7: Load dataset with error handling and create larger sample if needed\n",
    "print(\"\\nLoading dataset...\")\n",
    "file_path = 'banking_complaints_dataset1k.json'\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    print(f\"✓ Loaded {len(data)} records from {file_path}\")\n",
    "except:\n",
    "    print(\"✗ File not found. Creating expanded sample dataset...\")\n",
    "    # Create more diverse sample data for better training in standard Alpaca format\n",
    "    sample_data = [\n",
    "        {\n",
    "            \"instruction\": SYSTEM_PROMPT,  # Use the system prompt\n",
    "            \"input\": \"My credit card was charged twice for the same purchase at Target yesterday.\",\n",
    "            \"output\": json.dumps({\n",
    "                \"ticket_type\": \"complaint\", \n",
    "                \"title\": \"Duplicate credit card charge\", \n",
    "                \"description\": \"Customer was charged twice for the same purchase at Target\", \n",
    "                \"severity\": \"high\", \n",
    "                \"department_impacted\": \"Credit Card Services\", \n",
    "                \"service_impacted\": \"Credit Card\", \n",
    "                \"supporting_documents\": \"Receipt, credit card statement\", \n",
    "                \"preferred_communication\": \"Phone\"\n",
    "            })\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": SYSTEM_PROMPT,\n",
    "            \"input\": \"I need help understanding the fees on my mortgage statement.\",\n",
    "            \"output\": json.dumps({\n",
    "                \"ticket_type\": \"inquiry\", \n",
    "                \"title\": \"Mortgage statement clarification\", \n",
    "                \"description\": \"Customer needs explanation of mortgage statement fees\", \n",
    "                \"severity\": \"low\", \n",
    "                \"department_impacted\": \"Loans\", \n",
    "                \"service_impacted\": \"Mortgage\", \n",
    "                \"supporting_documents\": \"Mortgage statement\", \n",
    "                \"preferred_communication\": \"Email\"\n",
    "            })\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": SYSTEM_PROMPT,\n",
    "            \"input\": \"Someone used my debit card without permission and made several purchases.\",\n",
    "            \"output\": json.dumps({\n",
    "                \"ticket_type\": \"complaint\", \n",
    "                \"title\": \"Unauthorized debit card transactions\", \n",
    "                \"description\": \"Customer reports fraudulent debit card activity\", \n",
    "                \"severity\": \"critical\", \n",
    "                \"department_impacted\": \"Fraud Prevention\", \n",
    "                \"service_impacted\": \"Debit Card\", \n",
    "                \"supporting_documents\": \"Bank statements, police report\", \n",
    "                \"preferred_communication\": \"Phone\"\n",
    "            })\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": SYSTEM_PROMPT,\n",
    "            \"input\": \"I want to know about your business loan rates and application process.\",\n",
    "            \"output\": json.dumps({\n",
    "                \"ticket_type\": \"inquiry\", \n",
    "                \"title\": \"Business loan information request\", \n",
    "                \"description\": \"Customer inquiring about business loan rates and application process\", \n",
    "                \"severity\": \"low\", \n",
    "                \"department_impacted\": \"Loans\", \n",
    "                \"service_impacted\": \"Business Loans\", \n",
    "                \"supporting_documents\": \"None\", \n",
    "                \"preferred_communication\": \"Email\"\n",
    "            })\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": SYSTEM_PROMPT,\n",
    "            \"input\": \"My mobile app keeps crashing when I try to transfer money.\",\n",
    "            \"output\": json.dumps({\n",
    "                \"ticket_type\": \"assistance\", \n",
    "                \"title\": \"Mobile app technical issue\", \n",
    "                \"description\": \"Customer experiencing app crashes during money transfers\", \n",
    "                \"severity\": \"medium\", \n",
    "                \"department_impacted\": \"IT Support\", \n",
    "                \"service_impacted\": \"Mobile App\", \n",
    "                \"supporting_documents\": \"Screenshot of error\", \n",
    "                \"preferred_communication\": \"Chat\"\n",
    "            })\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": SYSTEM_PROMPT,\n",
    "            \"input\": \"I was charged an overdraft fee but I had money in my savings account.\",\n",
    "            \"output\": json.dumps({\n",
    "                \"ticket_type\": \"complaint\", \n",
    "                \"title\": \"Inappropriate overdraft fee\", \n",
    "                \"description\": \"Customer charged overdraft fee despite having funds in savings\", \n",
    "                \"severity\": \"medium\", \n",
    "                \"department_impacted\": \"Customer Service\", \n",
    "                \"service_impacted\": \"Overdraft Protection\", \n",
    "                \"supporting_documents\": \"Account statements\", \n",
    "                \"preferred_communication\": \"Phone\"\n",
    "            })\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": SYSTEM_PROMPT,\n",
    "            \"input\": \"Can you fix my computer? It won't turn on.\",\n",
    "            \"output\": json.dumps({\n",
    "                \"error\": \"Request is outside the banking support domain.\"\n",
    "            })\n",
    "        },\n",
    "        {\n",
    "            \"instruction\": SYSTEM_PROMPT,\n",
    "            \"input\": \"I need dating advice.\",\n",
    "            \"output\": json.dumps({\n",
    "                \"error\": \"Request is outside the banking support domain.\"\n",
    "            })\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Replicate to create more training data\n",
    "    data = sample_data * 50  # 400 samples total\n",
    "    print(f\"✓ Created {len(data)} sample records for testing\")\n",
    "\n",
    "# Step 8: Format dataset properly for standard Alpaca format\n",
    "formatted_dataset = []\n",
    "for item in data:\n",
    "    if \"instruction\" in item and \"input\" in item and \"output\" in item:\n",
    "        # Ensure instruction uses the system prompt\n",
    "        instruction = SYSTEM_PROMPT if item[\"instruction\"] != SYSTEM_PROMPT else item[\"instruction\"]\n",
    "        \n",
    "        # Ensure output is a JSON string\n",
    "        if isinstance(item[\"output\"], dict):\n",
    "            output_str = json.dumps(item[\"output\"])\n",
    "        else:\n",
    "            output_str = item[\"output\"]\n",
    "        \n",
    "        formatted_dataset.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"input\": item[\"input\"],\n",
    "            \"output\": output_str\n",
    "        })\n",
    "    elif \"input\" in item and \"output\" in item:\n",
    "        # Handle cases where instruction is missing - add system prompt\n",
    "        if isinstance(item[\"output\"], dict):\n",
    "            output_str = json.dumps(item[\"output\"])\n",
    "        else:\n",
    "            output_str = item[\"output\"]\n",
    "            \n",
    "        formatted_dataset.append({\n",
    "            \"instruction\": SYSTEM_PROMPT,\n",
    "            \"input\": item[\"input\"],\n",
    "            \"output\": output_str\n",
    "        })\n",
    "\n",
    "print(f\"✓ Formatted {len(formatted_dataset)} samples in standard Alpaca format\")\n",
    "\n",
    "# Step 9: Split dataset and save to files\n",
    "train_data, temp_data = train_test_split(formatted_dataset, test_size=0.3, random_state=42)\n",
    "eval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"✓ Train: {len(train_data)} samples\")\n",
    "print(f\"✓ Eval: {len(eval_data)} samples\") \n",
    "print(f\"✓ Test: {len(test_data)} samples\")\n",
    "\n",
    "# Save splits to separate JSON files\n",
    "print(f\"\\nSaving dataset splits to files...\")\n",
    "with open('train_data.json', 'w') as f:\n",
    "    json.dump(train_data, f, indent=2)\n",
    "    \n",
    "with open('eval_data.json', 'w') as f:\n",
    "    json.dump(eval_data, f, indent=2)\n",
    "    \n",
    "with open('test_data.json', 'w') as f:\n",
    "    json.dump(test_data, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved train_data.json ({len(train_data)} samples)\")\n",
    "print(f\"✓ Saved eval_data.json ({len(eval_data)} samples)\")\n",
    "print(f\"✓ Saved test_data.json ({len(test_data)} samples)\")\n",
    "\n",
    "# Step 10: Create HF datasets\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(eval_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "# Step 11: Format prompts correctly for standard Alpaca format\n",
    "EOS_TOKEN = tokenizer.eos_token if tokenizer.eos_token else \"</s>\"\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    texts = []\n",
    "    for i in range(len(examples[\"instruction\"])):\n",
    "        # Modified line: Directly concatenate instruction, input, and output (JSON)\n",
    "        # without the \"### Response:\" tag from the alpaca_prompt.\n",
    "        text = f\"{examples['instruction'][i]}\\n\\n### Input:\\n{examples['input'][i]}\\n{examples['output'][i]}\"\n",
    "        text = text + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Apply formatting\n",
    "train_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n",
    "eval_dataset = eval_dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Step 12: Save formatted datasets for inspection\n",
    "print(f\"\\nSaving formatted datasets...\")\n",
    "\n",
    "# Save the formatted text data so you can see what's being fed to the model\n",
    "formatted_train_samples = []\n",
    "formatted_eval_samples = []\n",
    "\n",
    "for i in range(min(10, len(train_dataset))):  # Save first 10 samples for inspection\n",
    "    formatted_train_samples.append({\n",
    "        \"raw_instruction\": train_data[i][\"instruction\"],\n",
    "        \"raw_input\": train_data[i][\"input\"],\n",
    "        \"raw_output\": train_data[i][\"output\"], \n",
    "        \"formatted_text\": train_dataset[i][\"text\"]\n",
    "    })\n",
    "\n",
    "for i in range(min(5, len(eval_dataset))):  # Save first 5 eval samples\n",
    "    formatted_eval_samples.append({\n",
    "        \"raw_instruction\": eval_data[i][\"instruction\"],\n",
    "        \"raw_input\": eval_data[i][\"input\"],\n",
    "        \"raw_output\": eval_data[i][\"output\"],\n",
    "        \"formatted_text\": eval_dataset[i][\"text\"]\n",
    "    })\n",
    "\n",
    "with open('formatted_train_samples.json', 'w') as f:\n",
    "    json.dump(formatted_train_samples, f, indent=2)\n",
    "    \n",
    "with open('formatted_eval_samples.json', 'w') as f:\n",
    "    json.dump(formatted_eval_samples, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved formatted_train_samples.json (first 10 training examples)\")\n",
    "print(f\"✓ Saved formatted_eval_samples.json (first 5 eval examples)\")\n",
    "print(f\"✓ You can now inspect these files to see exactly what the model will be trained on!\")\n",
    "\n",
    "print(\"\\n✓ Dataset preparation complete!\")\n",
    "print(f\"\\nSample formatted text (first 500 chars):\")\n",
    "print(train_dataset[0][\"text\"][:500] + \"...\")\n",
    "\n",
    "# Verify JSON structure\n",
    "print(f\"\\nVerifying data quality...\")\n",
    "try:\n",
    "    sample_output = json.loads(train_data[0][\"output\"])\n",
    "    print(\"✓ JSON structure is valid\")\n",
    "    print(f\"✓ Sample output keys: {list(sample_output.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ JSON structure issue: {e}\")\n",
    "\n",
    "# Final memory check\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Memory Usage:\")\n",
    "    print(f\"- Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"- Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f16326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Cell 4: Enhanced Training from Scratch with System Prompt\n",
    "import torch\n",
    "import gc\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, TrainerCallback\n",
    "from datasets import Dataset\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Clear GPU memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"=== ENHANCED TRAINING WITH SYSTEM PROMPT ===\\n\")\n",
    "\n",
    "# Define your SYSTEM PROMPT (same as in cell 3)\n",
    "SYSTEM_PROMPT = \"\"\"You are an automated banking customer service ticket analysis system. Your purpose is to parse a customer's request and structure it into a standardized JSON format for internal ticketing.\n",
    "\n",
    "You must perform the following actions:\n",
    "1. Carefully analyze the user's input to understand their intent and key details.\n",
    "2. Populate all fields in the JSON object based only on the user's text. Do not invent information.\n",
    "3. Adhere strictly to the defined categories for ticket_type, severity, and other categorical fields.\n",
    "4. If the user's request is NOT related to banking or financial services (e.g., tech support for a personal computer, dating advice), you MUST reject it by responding with {\"error\": \"Request is outside the banking support domain.\"}.\n",
    "5. Your entire response must be ONLY the JSON object, with no conversational text, apologies, or explanations.\n",
    "\n",
    "The required JSON format is:\n",
    "{\n",
    "    \"ticket_type\": \"complaint\" | \"inquiry\" | \"assistance\",\n",
    "    \"title\": \"A brief, descriptive summary of the user's issue.\",\n",
    "    \"description\": \"A more detailed description based on the user's full input.\",\n",
    "    \"severity\": \"low\" | \"medium\" | \"high\" | \"critical\",\n",
    "    \"department_impacted\": \"The most relevant bank department.\",\n",
    "    \"service_impacted\": \"The specific banking service affected.\",\n",
    "    \"supporting_documents\": \"Attached documents and files by the customer\",\n",
    "    \"preferred_communication\": \"preferred method to contact the customer\"\n",
    "}\"\"\"\n",
    "\n",
    "# Load base model (not the fine-tuned one)\n",
    "print(\"Loading base model...\")\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\",\n",
    "    max_seq_length=1024,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"sequential\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Add enhanced LoRA adapter with rank 32 from the start\n",
    "print(\"\\nAdding enhanced LoRA adapter (rank 32)...\")\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=32,  # Higher rank from the start\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=42,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# Load datasets\n",
    "print(\"\\nLoading datasets...\")\n",
    "with open('train_data.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('eval_data.json', 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "with open('test_data.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"✓ Loaded datasets - Train: {len(train_data)}, Eval: {len(eval_data)}, Test: {len(test_data)}\")\n",
    "\n",
    "# Format datasets\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token if tokenizer.eos_token else \"</s>\"\n",
    "\n",
    "# Create formatted datasets\n",
    "print(\"\\nFormatting datasets...\")\n",
    "train_texts = []\n",
    "eval_texts = []\n",
    "test_texts = []\n",
    "\n",
    "for item in train_data:\n",
    "    text = alpaca_prompt.format(\n",
    "        instruction=item[\"instruction\"],\n",
    "        input=item[\"input\"],\n",
    "        output=item[\"output\"]\n",
    "    ) + EOS_TOKEN\n",
    "    train_texts.append(text)\n",
    "\n",
    "for item in eval_data:\n",
    "    text = alpaca_prompt.format(\n",
    "        instruction=item[\"instruction\"],\n",
    "        input=item[\"input\"],\n",
    "        output=item[\"output\"]\n",
    "    ) + EOS_TOKEN\n",
    "    eval_texts.append(text)\n",
    "\n",
    "for item in test_data:\n",
    "    text = alpaca_prompt.format(\n",
    "        instruction=item[\"instruction\"],\n",
    "        input=item[\"input\"],\n",
    "        output=item[\"output\"]\n",
    "    ) + EOS_TOKEN\n",
    "    test_texts.append(text)\n",
    "\n",
    "train_dataset = Dataset.from_dict({\"text\": train_texts})\n",
    "eval_dataset = Dataset.from_dict({\"text\": eval_texts})\n",
    "test_dataset = Dataset.from_dict({\"text\": test_texts})\n",
    "\n",
    "# Enhanced callback for comprehensive metrics tracking\n",
    "class EnhancedLossCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.eval_losses = []\n",
    "        self.learning_rates = []\n",
    "        self.steps = []\n",
    "        self.eval_steps = []\n",
    "        \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None:\n",
    "            if \"loss\" in logs:\n",
    "                self.train_losses.append(logs[\"loss\"])\n",
    "                self.steps.append(state.global_step)\n",
    "            if \"eval_loss\" in logs:\n",
    "                self.eval_losses.append(logs[\"eval_loss\"])\n",
    "                self.eval_steps.append(state.global_step)\n",
    "            if \"learning_rate\" in logs:\n",
    "                self.learning_rates.append(logs[\"learning_rate\"])\n",
    "                \n",
    "    def plot_metrics(self, save_path=\"enhanced_training_metrics.png\"):\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Plot losses\n",
    "        if self.train_losses:\n",
    "            ax1.plot(self.steps, self.train_losses, 'b-', label='Training Loss', alpha=0.7)\n",
    "        if self.eval_losses:\n",
    "            ax1.plot(self.eval_steps, self.eval_losses, 'r-', label='Validation Loss', linewidth=2)\n",
    "        ax1.set_xlabel('Steps')\n",
    "        ax1.set_ylabel('Loss')\n",
    "        ax1.set_title('Training and Validation Loss - Enhanced Configuration')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_ylim(bottom=0)\n",
    "        \n",
    "        # Plot learning rate\n",
    "        if self.learning_rates:\n",
    "            ax2.plot(self.steps[:len(self.learning_rates)], self.learning_rates, 'g-', label='Learning Rate')\n",
    "            ax2.set_xlabel('Steps')\n",
    "            ax2.set_ylabel('Learning Rate')\n",
    "            ax2.set_title('Learning Rate Schedule')\n",
    "            ax2.legend()\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print statistics\n",
    "        if self.train_losses and self.eval_losses:\n",
    "            print(f\"\\nTraining Statistics:\")\n",
    "            print(f\"  - Initial training loss: {self.train_losses[0]:.4f}\")\n",
    "            print(f\"  - Final training loss: {self.train_losses[-1]:.4f}\")\n",
    "            print(f\"  - Best training loss: {min(self.train_losses):.4f}\")\n",
    "            print(f\"  - Initial validation loss: {self.eval_losses[0]:.4f}\")\n",
    "            print(f\"  - Final validation loss: {self.eval_losses[-1]:.4f}\")\n",
    "            print(f\"  - Best validation loss: {min(self.eval_losses):.4f}\")\n",
    "\n",
    "loss_callback = EnhancedLossCallback()\n",
    "\n",
    "# Optional W&B integration\n",
    "use_wandb = False  # Set to True to use Weights & Biases\n",
    "if use_wandb:\n",
    "    try:\n",
    "        import wandb\n",
    "        wandb.init(\n",
    "            project=\"banking-assistant-enhanced\",\n",
    "            name=f\"qwen-7b-rank32-{datetime.now().strftime('%Y%m%d_%H%M%S')}\",\n",
    "            config={\n",
    "                \"model\": \"Qwen2.5-7B-Instruct\",\n",
    "                \"lora_rank\": 32,\n",
    "                \"dataset_size\": len(train_data),\n",
    "                \"epochs\": 5,\n",
    "                \"batch_size\": 2,\n",
    "                \"learning_rate\": 1.5e-4,\n",
    "            }\n",
    "        )\n",
    "        report_to = \"wandb\"\n",
    "        print(\"✓ Weights & Biases initialized\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️ W&B not installed. Install with: pip install wandb\")\n",
    "        report_to = \"none\"\n",
    "else:\n",
    "    report_to = \"none\"\n",
    "\n",
    "# Optimized training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./banking_assistant_enhanced_v2\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=5,  # 5 epochs total with enhanced config\n",
    "    logging_steps=10,\n",
    "    eval_steps=30,\n",
    "    save_steps=60,  # Changed to be multiple of eval_steps (30 * 2 = 60)\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    learning_rate=1.5e-4,  # Balanced learning rate\n",
    "    fp16=True,\n",
    "    warmup_steps=100,  # Longer warmup for stability\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=42,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=report_to,\n",
    "    remove_unused_columns=True,\n",
    "    ddp_find_unused_parameters=False if torch.cuda.device_count() > 1 else None,\n",
    "    optim=\"adamw_torch\",\n",
    "    weight_decay=0.01,\n",
    "    adam_epsilon=1e-8,\n",
    "    max_grad_norm=1.0,\n",
    "    gradient_checkpointing=True,\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "print(\"\\nInitializing enhanced trainer...\")\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - LoRA rank: 32\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"  - Weight decay: {training_args.weight_decay}\")\n",
    "print(f\"  - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=training_args,\n",
    "    callbacks=[loss_callback],\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"\\nStarting enhanced training from scratch...\")\n",
    "total_steps = len(train_dataset) // training_args.per_device_train_batch_size // training_args.gradient_accumulation_steps * training_args.num_train_epochs\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "\n",
    "try:\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    \n",
    "    # Plot comprehensive metrics\n",
    "    print(\"\\nPlotting training metrics...\")\n",
    "    loss_callback.plot_metrics(save_path=f\"enhanced_metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png\")\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"\\nSaving enhanced model...\")\n",
    "    model.save_pretrained(\"banking_assistant_enhanced_final\")\n",
    "    tokenizer.save_pretrained(\"banking_assistant_enhanced_final\")\n",
    "    \n",
    "    # Save merged version\n",
    "    print(\"\\nSaving merged model...\")\n",
    "    model.save_pretrained_merged(\"banking_assistant_enhanced_merged_final\", tokenizer, save_method=\"merged_16bit\")\n",
    "    \n",
    "    print(\"\\n✓ Training completed successfully!\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nTraining interrupted by user.\")\n",
    "    print(\"Saving checkpoint...\")\n",
    "    model.save_pretrained(\"banking_assistant_enhanced_checkpoint\")\n",
    "    tokenizer.save_pretrained(\"banking_assistant_enhanced_checkpoint\")\n",
    "    loss_callback.plot_metrics(save_path=\"enhanced_metrics_interrupted.png\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ Training error: {e}\")\n",
    "    raise\n",
    "\n",
    "# Custom evaluation function for test set\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def evaluate_on_test_set(model, tokenizer, test_data, batch_size=4):\n",
    "    \"\"\"Custom evaluation function with proper loss calculation\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    print(f\"Evaluating {len(test_data)} test samples...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(test_data), batch_size):\n",
    "            batch_texts = []\n",
    "            for j in range(i, min(i+batch_size, len(test_data))):\n",
    "                text = alpaca_prompt.format(\n",
    "                    instruction=test_data[j][\"instruction\"],\n",
    "                    input=test_data[j][\"input\"],\n",
    "                    output=test_data[j][\"output\"]\n",
    "                ) + EOS_TOKEN\n",
    "                batch_texts.append(text)\n",
    "            \n",
    "            # Tokenize batch\n",
    "            encodings = tokenizer(\n",
    "                batch_texts,\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=1024,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            # Create labels (same as input_ids for language modeling)\n",
    "            labels = encodings[\"input_ids\"].clone()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(**encodings, labels=labels)\n",
    "            total_loss += outputs.loss.item() * len(batch_texts)\n",
    "            total_samples += len(batch_texts)\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (i // batch_size + 1) % 10 == 0:\n",
    "                print(f\"  Evaluated {min(i+batch_size, len(test_data))}/{len(test_data)} samples...\")\n",
    "    \n",
    "    avg_loss = total_loss / total_samples\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return avg_loss, perplexity\n",
    "\n",
    "# Run evaluation\n",
    "test_loss, test_perplexity = evaluate_on_test_set(model, tokenizer, test_data)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Perplexity: {test_perplexity:.2f}\")\n",
    "\n",
    "# Generate sample predictions\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Enable inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Diverse test cases - including non-banking requests\n",
    "test_cases = [\n",
    "    {\n",
    "        \"input\": \"My debit card was declined at the ATM but I have sufficient funds.\",\n",
    "        \"expected_type\": \"complaint\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I'd like information about opening a business checking account.\",\n",
    "        \"expected_type\": \"inquiry\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Someone made unauthorized purchases with my credit card in another country.\",\n",
    "        \"expected_type\": \"complaint\",\n",
    "        \"expected_severity\": \"critical\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Can you help me set up automatic bill payments?\",\n",
    "        \"expected_type\": \"assistance\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"The mobile app crashes every time I try to check my balance.\",\n",
    "        \"expected_type\": \"assistance\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Can you help me fix my laptop? It won't start.\",\n",
    "        \"expected_output\": \"error\",\n",
    "        \"expected_error\": True\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"I need relationship advice.\",\n",
    "        \"expected_output\": \"error\",\n",
    "        \"expected_error\": True\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16123b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. upload adapters \n",
    "# Upload LoRA Adapters to Hugging Face\n",
    "import os\n",
    "from huggingface_hub import HfApi, create_repo, upload_folder\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "\n",
    "# Configuration\n",
    "ADAPTER_PATH = \"banking_assistant_enhanced_final\"\n",
    "HF_REPO_NAME = \"LaythAbuJafar/QwenInstruct_Agent1_Adapters\"\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # Replace with your actual token\n",
    "\n",
    "print(\"=== Uploading LoRA Adapters to Hugging Face ===\\n\")\n",
    "\n",
    "# Step 1: Login to Hugging Face\n",
    "print(\"Logging in to Hugging Face...\")\n",
    "login(token=HF_TOKEN)\n",
    "print(\"✓ Logged in successfully\")\n",
    "\n",
    "# Step 2: Create or verify repository exists\n",
    "api = HfApi()\n",
    "try:\n",
    "    # Try to create the repo (will fail if it already exists, which is fine)\n",
    "    create_repo(\n",
    "        repo_id=HF_REPO_NAME,\n",
    "        repo_type=\"model\",\n",
    "        private=False,  # Set to True if you want a private repo\n",
    "        exist_ok=True\n",
    "    )\n",
    "    print(f\"✓ Repository '{HF_REPO_NAME}' is ready\")\n",
    "except Exception as e:\n",
    "    print(f\"Repository already exists or error: {e}\")\n",
    "\n",
    "# Step 3: Create a comprehensive model card\n",
    "print(\"\\nCreating model card...\")\n",
    "model_card_content = \"\"\"---\n",
    "base_model: unsloth/Qwen2.5-7B-Instruct-bnb-4bit\n",
    "tags:\n",
    "- banking\n",
    "- customer-service\n",
    "- json-generation\n",
    "- lora\n",
    "- qwen2.5\n",
    "- unsloth\n",
    "license: apache-2.0\n",
    "language:\n",
    "- en\n",
    "pipeline_tag: text-generation\n",
    "library_name: peft\n",
    "---\n",
    "\n",
    "# QwenInstruct Banking Agent - LoRA Adapters\n",
    "\n",
    "This model is a fine-tuned version of Qwen2.5-7B-Instruct for banking customer service ticket generation.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model**: unsloth/Qwen2.5-7B-Instruct-bnb-4bit\n",
    "- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)\n",
    "- **LoRA Rank**: 32\n",
    "- **LoRA Alpha**: 32\n",
    "- **Target Modules**: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj\n",
    "- **Training Framework**: Unsloth + TRL\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Dataset Size**: 1,000 banking customer service examples\n",
    "- **Training Split**: 70% train, 15% validation, 15% test\n",
    "- **Epochs**: 5\n",
    "- **Learning Rate**: 1.5e-4\n",
    "- **Batch Size**: 8 (2 * 4 gradient accumulation)\n",
    "- **Optimizer**: AdamW\n",
    "- **Weight Decay**: 0.01\n",
    "\n",
    "## Performance\n",
    "\n",
    "- **Final Training Loss**: 0.055\n",
    "- **Final Validation Loss**: 0.072\n",
    "- **JSON Generation Success Rate**: 100%\n",
    "- **Test Perplexity**: 8.32\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Loading the Adapters\n",
    "\n",
    "```python\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# Load base model\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\",\n",
    "    max_seq_length=1024,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Load LoRA adapters\n",
    "model = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"LaythAbuJafar/QwenInstruct_Agent1_Adapters\",\n",
    "    max_seq_length=1024,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Enable for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "```\n",
    "\n",
    "### Generation Example\n",
    "\n",
    "```python\n",
    "# Define the prompt template\n",
    "alpaca_prompt = \\\"\\\"\\\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "# Create prompt\n",
    "prompt = alpaca_prompt.format(\n",
    "    instruction=\"You are a banking customer service assistant. Analyze the customer input and create a complaint/inquiry ticket in valid JSON format. The JSON must include these fields: ticket_type, title, description, severity, department_impacted, service_impacted, supporting_documents, preferred_communication.\",\n",
    "    input=\"My credit card was charged twice for the same purchase.\"\n",
    ")\n",
    "\n",
    "# Generate\n",
    "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "```\n",
    "\n",
    "## Expected Output Format\n",
    "\n",
    "The model generates JSON tickets with the following structure:\n",
    "```json\n",
    "{\n",
    "    \"ticket_type\": \"complaint|inquiry|assistance\",\n",
    "    \"title\": \"Brief description of the issue\",\n",
    "    \"description\": \"Detailed explanation of the customer's request\",\n",
    "    \"severity\": \"low|medium|high|critical\",\n",
    "    \"department_impacted\": \"Relevant department\",\n",
    "    \"service_impacted\": \"Specific service affected\",\n",
    "    \"supporting_documents\": \"Required documentation\",\n",
    "    \"preferred_communication\": \"phone|email|chat|not specified\"\n",
    "}\n",
    "```\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Trained specifically for banking domain\n",
    "- English language only\n",
    "- Requires the exact base model (unsloth/Qwen2.5-7B-Instruct-bnb-4bit)\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this model, please cite:\n",
    "```\n",
    "@misc{qweninstruct-banking-agent,\n",
    "  author = {Layth Abu Jafar},\n",
    "  title = {QwenInstruct Banking Agent},\n",
    "  year = {2024},\n",
    "  publisher = {Hugging Face},\n",
    "  url = {https://huggingface.co/LaythAbuJafar/QwenInstruct_Agent1_Adapters}\n",
    "}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(os.path.join(ADAPTER_PATH, \"README.md\"), \"w\") as f:\n",
    "    f.write(model_card_content)\n",
    "print(\"✓ Model card created\")\n",
    "\n",
    "# Step 4: Create adapter info file\n",
    "adapter_info = {\n",
    "    \"base_model\": \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\",\n",
    "    \"model_type\": \"qwen2.5\",\n",
    "    \"fine_tuning_method\": \"lora\",\n",
    "    \"lora_rank\": 32,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    \"training_framework\": \"unsloth\",\n",
    "    \"dataset_size\": 1000,\n",
    "    \"training_epochs\": 5,\n",
    "    \"final_loss\": 0.055,\n",
    "    \"validation_loss\": 0.072\n",
    "}\n",
    "\n",
    "with open(os.path.join(ADAPTER_PATH, \"training_info.json\"), \"w\") as f:\n",
    "    json.dump(adapter_info, f, indent=2)\n",
    "print(\"✓ Training info saved\")\n",
    "\n",
    "# Step 5: Upload to Hugging Face\n",
    "print(f\"\\nUploading adapters to {HF_REPO_NAME}...\")\n",
    "try:\n",
    "    api.upload_folder(\n",
    "        folder_path=ADAPTER_PATH,\n",
    "        repo_id=HF_REPO_NAME,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload fine-tuned LoRA adapters for banking customer service\"\n",
    "    )\n",
    "    print(f\"✓ Successfully uploaded to https://huggingface.co/{HF_REPO_NAME}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error uploading: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Make sure you have write access to the repository\")\n",
    "    print(\"2. Check that your HF token has 'write' permissions\")\n",
    "    print(\"3. Verify the repository name is correct\")\n",
    "\n",
    "print(\"\\n=== Upload Complete ===\")\n",
    "print(f\"Your adapters are now available at: https://huggingface.co/{HF_REPO_NAME}\")\n",
    "print(\"\\nTo use these adapters:\")\n",
    "print(f\"model = FastLanguageModel.from_pretrained('{HF_REPO_NAME}', ...)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa90cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Upload Merged Model to Hugging Face\n",
    "import os\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "from huggingface_hub import login\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Configuration\n",
    "MERGED_MODEL_PATH = \"banking_assistant_enhanced_merged_final\"\n",
    "HF_REPO_NAME = \"LaythAbuJafar/QwenInstruct_Agent1_Merged\"\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\"  # Replace with your actual token\n",
    "\n",
    "print(\"=== Uploading Merged Model to Hugging Face ===\\n\")\n",
    "\n",
    "# Step 1: Login to Hugging Face\n",
    "print(\"Logging in to Hugging Face...\")\n",
    "login(token=HF_TOKEN)\n",
    "print(\"✓ Logged in successfully\")\n",
    "\n",
    "# Step 2: Create or verify repository exists\n",
    "api = HfApi()\n",
    "try:\n",
    "    create_repo(\n",
    "        repo_id=HF_REPO_NAME,\n",
    "        repo_type=\"model\",\n",
    "        private=False,  # Set to True if you want a private repo\n",
    "        exist_ok=True\n",
    "    )\n",
    "    print(f\"✓ Repository '{HF_REPO_NAME}' is ready\")\n",
    "except Exception as e:\n",
    "    print(f\"Repository already exists or error: {e}\")\n",
    "\n",
    "# Step 3: Create a comprehensive model card for the merged model\n",
    "print(\"\\nCreating model card...\")\n",
    "model_card_content = \"\"\"---\n",
    "base_model: unsloth/Qwen2.5-7B-Instruct-bnb-4bit\n",
    "tags:\n",
    "- banking\n",
    "- customer-service  \n",
    "- json-generation\n",
    "- merged-model\n",
    "- qwen2.5\n",
    "- unsloth\n",
    "- text-generation\n",
    "license: apache-2.0\n",
    "language:\n",
    "- en\n",
    "pipeline_tag: text-generation\n",
    "library_name: transformers\n",
    "---\n",
    "\n",
    "# QwenInstruct Banking Agent - Merged Model\n",
    "\n",
    "This is a merged version of the fine-tuned Qwen2.5-7B-Instruct model for banking customer service ticket generation. The LoRA adapters have been merged into the base model for easier deployment.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model**: unsloth/Qwen2.5-7B-Instruct-bnb-4bit\n",
    "- **Fine-tuning Method**: LoRA (merged into base model)\n",
    "- **Model Format**: 16-bit merged model\n",
    "- **Model Size**: ~15 GB\n",
    "- **Training Framework**: Unsloth + TRL\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **LoRA Configuration**:\n",
    "  - Rank: 32\n",
    "  - Alpha: 32\n",
    "  - Dropout: 0.1\n",
    "  - Target Modules: q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj\n",
    "\n",
    "- **Training Configuration**:\n",
    "  - Dataset Size: 1,000 banking examples\n",
    "  - Epochs: 5\n",
    "  - Learning Rate: 1.5e-4\n",
    "  - Batch Size: 8 (effective)\n",
    "  - Optimizer: AdamW\n",
    "  - Weight Decay: 0.01\n",
    "\n",
    "## Performance Metrics\n",
    "\n",
    "- **Final Training Loss**: 0.055\n",
    "- **Final Validation Loss**: 0.072  \n",
    "- **JSON Generation Success Rate**: 100%\n",
    "- **Test Perplexity**: 8.32\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Simple Loading\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the merged model directly\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"LaythAbuJafar/QwenInstruct_Agent1_Merged\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LaythAbuJafar/QwenInstruct_Agent1_Merged\")\n",
    "```\n",
    "\n",
    "### Using with Unsloth\n",
    "\n",
    "```python\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Load with Unsloth for optimized inference\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"LaythAbuJafar/QwenInstruct_Agent1_Merged\",\n",
    "    max_seq_length=1024,\n",
    "    dtype=torch.float16,\n",
    "    load_in_4bit=True,  # Optional: use 4-bit quantization\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)\n",
    "```\n",
    "\n",
    "### Generation Example\n",
    "\n",
    "```python\n",
    "# Define the prompt template\n",
    "alpaca_prompt = \\\"\\\"\\\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "\\\"\\\"\\\"\n",
    "\n",
    "# Example usage\n",
    "instruction = \"You are a banking customer service assistant. Analyze the customer input and create a complaint/inquiry ticket in valid JSON format. The JSON must include these fields: ticket_type, title, description, severity, department_impacted, service_impacted, supporting_documents, preferred_communication.\"\n",
    "user_input = \"I received a suspicious email asking for my banking credentials.\"\n",
    "\n",
    "prompt = alpaca_prompt.format(\n",
    "    instruction=instruction,\n",
    "    input=user_input\n",
    ")\n",
    "\n",
    "# Generate response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.1,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "# Extract JSON from response\n",
    "json_start = response.find(\"### Response:\") + len(\"### Response:\")\n",
    "json_response = response[json_start:].strip()\n",
    "print(json_response)\n",
    "```\n",
    "\n",
    "## Expected Output Format\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"ticket_type\": \"complaint|inquiry|assistance\",\n",
    "    \"title\": \"Brief issue description\",\n",
    "    \"description\": \"Detailed customer request explanation\",\n",
    "    \"severity\": \"low|medium|high|critical\",\n",
    "    \"department_impacted\": \"Relevant department\",\n",
    "    \"service_impacted\": \"Affected service\",\n",
    "    \"supporting_documents\": \"Required documents\",\n",
    "    \"preferred_communication\": \"phone|email|chat|not specified\"\n",
    "}\n",
    "```\n",
    "\n",
    "## Example Outputs\n",
    "\n",
    "**Input**: \"My credit card payment didn't go through but the money was deducted.\"\n",
    "```json\n",
    "{\n",
    "    \"ticket_type\": \"complaint\",\n",
    "    \"title\": \"Failed payment with deduction\",\n",
    "    \"description\": \"Customer reports credit card payment failed but money was deducted from account\",\n",
    "    \"severity\": \"high\",\n",
    "    \"department_impacted\": \"Payment Processing\",\n",
    "    \"service_impacted\": \"Credit Card Payments\",\n",
    "    \"supporting_documents\": \"Transaction history, payment confirmation\",\n",
    "    \"preferred_communication\": \"phone\"\n",
    "}\n",
    "```\n",
    "\n",
    "## Deployment Tips\n",
    "\n",
    "1. **Memory Requirements**: ~15 GB for full precision, ~8 GB with 4-bit quantization\n",
    "2. **Inference Speed**: Use Flash Attention 2 for faster inference\n",
    "3. **Batch Processing**: Model supports batch inference for multiple tickets\n",
    "4. **Temperature**: Use 0.5-0.7 for consistent JSON generation\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Specialized for banking domain only\n",
    "- English language support only\n",
    "- Requires GPU for optimal performance\n",
    "- JSON structure is fixed to the trained format\n",
    "\n",
    "## Citation\n",
    "\n",
    "```bibtex\n",
    "@misc{qweninstruct-banking-agent-merged,\n",
    "  author = {Layth Abu Jafar},\n",
    "  title = {QwenInstruct Banking Agent - Merged Model},\n",
    "  year = {2024},\n",
    "  publisher = {Hugging Face},\n",
    "  url = {https://huggingface.co/LaythAbuJafar/QwenInstruct_Agent1_Merged}\n",
    "}\n",
    "```\n",
    "\n",
    "## Acknowledgments\n",
    "\n",
    "- Base model: Qwen Team\n",
    "- Fine-tuning framework: Unsloth\n",
    "- Training library: TRL (Transformer Reinforcement Learning)\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(os.path.join(MERGED_MODEL_PATH, \"README.md\"), \"w\") as f:\n",
    "    f.write(model_card_content)\n",
    "print(\"✓ Model card created\")\n",
    "\n",
    "# Step 4: Create model info file\n",
    "model_info = {\n",
    "    \"model_type\": \"qwen2.5-merged\",\n",
    "    \"base_model\": \"unsloth/Qwen2.5-7B-Instruct-bnb-4bit\",\n",
    "    \"merge_method\": \"unsloth_merged_16bit\",\n",
    "    \"fine_tuning_details\": {\n",
    "        \"method\": \"lora\",\n",
    "        \"rank\": 32,\n",
    "        \"alpha\": 32,\n",
    "        \"modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    },\n",
    "    \"performance\": {\n",
    "        \"final_loss\": 0.055,\n",
    "        \"validation_loss\": 0.072,\n",
    "        \"json_success_rate\": \"100%\",\n",
    "        \"test_perplexity\": 8.32\n",
    "    },\n",
    "    \"training_framework\": \"unsloth\",\n",
    "    \"model_size\": \"~15GB\",\n",
    "    \"quantization_compatible\": True\n",
    "}\n",
    "\n",
    "with open(os.path.join(MERGED_MODEL_PATH, \"model_info.json\"), \"w\") as f:\n",
    "    json.dump(model_info, f, indent=2)\n",
    "print(\"✓ Model info saved\")\n",
    "\n",
    "# Step 5: Check model files\n",
    "print(\"\\nChecking model files...\")\n",
    "model_files = os.listdir(MERGED_MODEL_PATH)\n",
    "print(f\"Found {len(model_files)} files to upload:\")\n",
    "for file in model_files[:10]:  # Show first 10 files\n",
    "    file_path = os.path.join(MERGED_MODEL_PATH, file)\n",
    "    size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    print(f\"  - {file}: {size_mb:.1f} MB\")\n",
    "if len(model_files) > 10:\n",
    "    print(f\"  ... and {len(model_files) - 10} more files\")\n",
    "\n",
    "# Step 6: Upload to Hugging Face\n",
    "print(f\"\\n⚠️  WARNING: This will upload ~15GB of data!\")\n",
    "print(f\"Uploading to {HF_REPO_NAME}...\")\n",
    "\n",
    "try:\n",
    "    # For large models, it's better to upload in chunks\n",
    "    # First, let's check the total size\n",
    "    total_size = sum(os.path.getsize(os.path.join(MERGED_MODEL_PATH, f)) \n",
    "                     for f in os.listdir(MERGED_MODEL_PATH))\n",
    "    total_size_gb = total_size / (1024**3)\n",
    "    print(f\"Total size to upload: {total_size_gb:.2f} GB\")\n",
    "    \n",
    "    # Upload the model\n",
    "    api.upload_folder(\n",
    "        folder_path=MERGED_MODEL_PATH,\n",
    "        repo_id=HF_REPO_NAME,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload merged Qwen2.5-7B banking assistant model\",\n",
    "        # For large uploads, you might want to add:\n",
    "        # multi_commits=True,\n",
    "        # multi_commits_verbose=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ Successfully uploaded to https://huggingface.co/{HF_REPO_NAME}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error uploading: {e}\")\n",
    "    print(\"\\nTroubleshooting for large model uploads:\")\n",
    "    print(\"1. Make sure you have a stable internet connection\")\n",
    "    print(\"2. Consider using git-lfs directly for very large files:\")\n",
    "    print(\"   - git clone https://huggingface.co/YOUR_REPO\")\n",
    "    print(\"   - git lfs track '*.bin' '*.safetensors'\")\n",
    "    print(\"   - git add . && git commit -m 'Add model'\")\n",
    "    print(\"   - git push\")\n",
    "    print(\"3. You can also use the huggingface-cli:\")\n",
    "    print(f\"   huggingface-cli upload {HF_REPO_NAME} {MERGED_MODEL_PATH}\")\n",
    "\n",
    "print(\"\\n=== Upload Complete ===\")\n",
    "print(f\"Your merged model is now available at: https://huggingface.co/{HF_REPO_NAME}\")\n",
    "print(\"\\nUsers can now load your model with:\")\n",
    "print(f\"model = AutoModelForCausalLM.from_pretrained('{HF_REPO_NAME}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "102d65ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: C:\\Users\\Laith\\Desktop\\qwen_instruct\\llama.cpp\\build\\bin\\Release\\llama-quantize.exe C:\\Users\\Laith\\Desktop\\qwen_instruct\\QwenInstruct_Agent1_Merged.gguf C:\\Users\\Laith\\Desktop\\qwen_instruct\\QwenInstruct_Agent1_Merged.q4_K_M.gguf q4_K_M\n",
      "✅ Success! Quantized model saved to:\n",
      "C:\\Users\\Laith\\Desktop\\qwen_instruct\\QwenInstruct_Agent1_Merged.q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "#7. quantize after gguf \n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "model_path = r\"C:\\Users\\Laith\\Desktop\\qwen_instruct\\QwenInstruct_Agent1_Merged.gguf\"\n",
    "quant_type = \"q4_K_M\"  # Options: q4_0, q5_1, q8_0, etc.\n",
    "llama_cpp_bin_dir = r\"C:\\Users\\Laith\\Desktop\\qwen_instruct\\llama.cpp\\build\\bin\\Release\"\n",
    "output_path = os.path.splitext(model_path)[0] + f\".{quant_type}.gguf\"\n",
    "\n",
    "# === MAIN LOGIC ===\n",
    "quantize_exe = os.path.join(llama_cpp_bin_dir, \"llama-quantize.exe\")\n",
    "if not os.path.exists(quantize_exe):\n",
    "    raise FileNotFoundError(f\"llama-quantize.exe not found at: {quantize_exe}\")\n",
    "if not os.path.exists(model_path):\n",
    "    raise FileNotFoundError(f\"Model file not found at: {model_path}\")\n",
    "\n",
    "cmd = [quantize_exe, model_path, output_path, quant_type]\n",
    "print(f\"Running: {' '.join(cmd)}\")\n",
    "\n",
    "result = subprocess.run(cmd)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(f\"✅ Success! Quantized model saved to:\\n{output_path}\")\n",
    "else:\n",
    "    print(f\"❌ Quantization failed with code {result.returncode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fbb3fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9c845cfdcd4bc08af5c4db54e0ba74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QwenInstruct_Agent1_Merged.q4_K_M.gguf:   0%|          | 0.00/4.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upload complete!\n"
     ]
    }
   ],
   "source": [
    "#upload quantized model to hf\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "local_path = r\"C:\\Users\\Laith\\Desktop\\qwen_instruct\\QwenInstruct_Agent1_Merged.q4_K_M.gguf\"\n",
    "repo_id = \"LaythAbuJafar/Qwen_Insturct7B_Agent1_GGUF_Q\"\n",
    "target_path_in_repo = \"QwenInstruct_Agent1_Merged.q4_K_M.gguf\"\n",
    "\n",
    "api.upload_file(\n",
    "    path_or_fileobj=local_path,\n",
    "    path_in_repo=target_path_in_repo,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(\"✅ Upload complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
